![Caption for Fig 1](ddqn_trajectory/algo_meanScore_heatmap.png)
*Caption for Fig 1: Heatmap of all algorithms. This heatmap visually represents the performance of various algorithms, with each algorithm ordered from left to right based on its validation performance, averaged across datasets. The colors used to indicate algorithm families are: Blue=SVD, Red=Adaboost, Purple=KNN, Green=MultiLayer Perceptron. The block sturcture observed in this heatmap suggest a performance trend where blue outperforms red, red outperforms purple, and purple outperforms green}*

![Caption for Fig 2](ddqn_trajectory/ddqn_trajectory_21.svg)
*Caption for Fig 2: DDQN action trajectory during meta-testing of the final phase, Round 2. The performance revealed with each action (solid line: validation performance, dashed line: train performance) is traced as a function of time. Each color labels the learning curve of an algorithm, indicated by their action numbers 0 - 39. The transparency of learning curves corresponds to the mean performance of the algorithm (darker is better). We observe the DDQN learns to start with good candidates (blue algorithms, ordered first in Fig. 1, and switch/continue training promising ones when achieving a performance plateau (e.g. in (a),  the agent switches from alg. 37 to previously partially trained alg. 36 around $t=exp(4)$; in (b) the agent switches several times to find better performance). Observations with overfitting, especially when trained with a small amount of time in early steps, (perfect train performance and low validation performance) can confuse the agent to overestimate an action (e.g. the choice of actions 3 and 26 in c). *